---
title: "Project 2"
author: "Tommy King and Steph Camino"
date: '2022-06-30'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, warning=FALSE, message=FALSE}
library(tidyverse)
```

# Introduction  
  
For this project, we're going to be taking a look at news article popularity data in the hopes of gaining some insight into the key factors that lead to articles accumulating shares on social media. To start out, we're going to hone in on the Entertainment data channel to do some exploration before automating our process for use with any of the channels. In the Entertainment subset of the data, we'll initially take a look at the variables corresponding to the day of the week an article was published on, as well as the number of words in the title/body of the article and the number of images and videos included.  
  
Once we've completed our exploratory analysis, we'll select features that we think would be a good fit in various types of models. We'll compare and evaluate the models we build on several metrics to decide which model provides the most accurate forecasts of article popularity.  
  

# Data

```{r, message=FALSE}
# URL for the Online News Popularity Data Folder
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/00332/OnlineNewsPopularity.zip"

# Creates two temporary files
temp <- tempfile()
temp2 <- tempfile()

# Downloads the zipped folder from the URL and saves it in temp
download.file(url, temp)

# Unzips temp and saves it in temp2
unzip(zipfile = temp, exdir = temp2)

# Reads in the data from temp2 and saves it as data
data <- readr::read_csv(file.path(temp2, "OnlineNewsPopularity/OnlineNewsPopularity.csv"))

# Unlinks our temporary files
unlink(c(temp, temp2), force = TRUE)
```

```{r}
# Creates a new dataset "entertainment"
# Filters data for when the data channel is entertainment
# Removes Variables that all start with data_channel_is_, url, and timedelta
# url and timedelta are non-predictive
entertainment <- data %>% as_tibble() %>%
  filter(data_channel_is_entertainment == 1) %>%
  select(-c(url, timedelta, starts_with("data_channel_is_")))

# Creates a new character variable "day" that states what day it is. Derived from the binary variables for each day. 
# Factors the variable so we can use it as a categorical variable.
# Orders the factors in the order of the days of the week.
entertainment2 <- entertainment
entertainment2$day <- ifelse(entertainment$weekday_is_monday == 1, "Monday",
                            ifelse(entertainment$weekday_is_tuesday == 1, "Tuesday",
                                   ifelse(entertainment$weekday_is_wednesday == 1, "Wednesday",
                                          ifelse(entertainment$weekday_is_thursday == 1, "Thursday",
                                                 ifelse(entertainment$weekday_is_friday == 1, "Friday",
                                                        ifelse(entertainment$weekday_is_saturday == 1, "Saturday",
                                                               ifelse(entertainment$weekday_is_sunday == 1, "Sunday", "NA"))))))) %>%
                      as.factor() %>% 
                      ordered(levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))
```
  

# Summarizations

```{r plots}

# Histogram for shares
g <- ggplot(entertainment, aes(x = shares)) + 
        geom_histogram(fill = "blue", binwidth = 5000) + 
        labs(x = "Shares", y = "Count", title = "Histogram of Shares")

# Scatterplot for n_tokens_contnt vs shares
g2 <- ggplot(entertainment2, aes(x = n_tokens_content, y = shares)) + 
          geom_point(color = "Green") + 
          labs(x = "Number of Words in the Content", y = "Shares", title = "Content Word Count vs Shares") 

# Scatterplot for n_tokens_title vs shares
g3 <- ggplot(entertainment2, aes(x = n_tokens_title, y = shares)) + 
          geom_point() + 
          labs(x = "Number of Words in the Title", y = "Shares", title = "Title Word Count vs Shares") 

# Added some new plots for number of images, videos and links vs. shares
g4 <- ggplot(entertainment, aes(x = num_imgs, y = shares)) + geom_point(color = "Red")
g5 <- ggplot(entertainment, aes(x = num_videos, y = shares)) + geom_point(color = "Aquamarine")
g6 <- ggplot(entertainment, aes(x = num_hrefs, y = shares)) + geom_point(color = "Purple")

# We can move this up to where we have all the package info eventually but wanted to put it here for now
# Can put all of our scatter plots into a single grid so they're all lined up!
library(gridExtra)

grid.arrange(g, g2, g3, g4, g5, g6, ncol = 2, nrow = 3)

# Looking into plotting some of the distributions of the more obscure variables
g7 <- ggplot(entertainment, aes(global_subjectivity)) + 
  geom_density(kernel = "gaussian", color = "Coral", fill = "Coral", alpha = .5)

g8 <- ggplot(entertainment, aes(global_sentiment_polarity)) + 
  geom_density(kernel = "gaussian", color = "Blue", fill = "Blue", alpha = .5)

g9 <- ggplot(entertainment, aes(global_rate_positive_words)) + 
  geom_density(kernel = "gaussian", color = "Green", fill = "Green", alpha = .5)

g10 <- ggplot(entertainment2, aes(x = day, y = shares)) + geom_boxplot(fill = "grey")

g7
g8
g9
g10
g11 <- ggplot(entertainment2, aes(x = n_tokens_content, y = shares)) + geom_point() + facet_wrap(~ day)
g11
```

```{r num_summaries}
#NOTE! Here I was looking at all of the summary statistics and correlations for the data after removing the binary and categorical variables but it was A LOT

#forSummary <- entertainment %>% select(-c(starts_with("weekday_is_"), is_weekend, day))
#summary(forSummary)
#cor(forSummary)
```


# Modeling  
  
## Train/Test Split  
  
The code chunk below splits our data into our training and testing sets using `caret`:  
  
```{r}
# can move this eventually
library(caret)

set.seed(1024)

train_index <-createDataPartition(entertainment$shares, p = 0.7, list = FALSE)

ent_train <- entertainment[train_index, ]
ent_test <- entertainment[-train_index, ]


```
  
## Linear Models  
  
```{r steph_model_l}
fitMLR1 <- lm(shares ~ .^2, entertainment)
```

```{r tommy_model_l}
```
  
## Ensamble Models 

### Random Forest Model

Random forest is a an ensemble learning method that is an extension of the idea of the bagging method. Like the bagging method, the random forest algorithm uses bagging, also known as bootstrap aggregation, to resample from the data or a fitted model randomly. Then multiple decision trees are created from these samples to create an uncorrelated forest and the results are then averaged. Unlike bagging, random forest doesn't use all of it's predictors but uses a random subset of predictors for each bootstrap sample. If there is a strong predictor, it'll likely be used for every first split in bagging, so randomly subsetting predictors will reduce correlation of tree predictions in random forest models. 
  
```{r steph_model_e}
# Training the Random Forest model with 2 fold cross-validation with center and scaling the data via preprocess, and considering the values of mtry of 1 to a third of number of columns in the data with tuneGrid.
fitRF <- train(shares ~ ., data = ent_train,
                method = "rf",
                preProcess = c("center", "scale"),
                trControl = trainControl(method = "cv", number = 2))
               
               #tuneGrid = expand.grid(mtry = c(1:round(ncol(entertainment)/3))))

```
  
```{r tommy_model_e}
```

# Comparison
