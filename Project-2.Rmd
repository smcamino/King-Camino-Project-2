---
title: "Project 2"
author: "Tommy King and Steph Camino"
date: '2022-06-30'
params:
  data_channel: "entertainment"
---


```{r, eval=FALSE}

renderReport <- function(type){
  rmarkdown::render("Project2.Rmd",
                    params = list(type = type),
                    output_format = "github_document",
                    output_options = list(html_preview = FALSE),
                    output_file = paste0("Project2-", type, ".html")
                    )
}

```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(knitr)
```

# Introduction  
  
For this project, we're going to be taking a look at news article popularity data in the hopes of gaining some insight into the key factors that lead to articles accumulating shares on social media. To start out, we're going to hone in on the Entertainment data channel to do some exploration before automating our process for use with any of the channels. In the Entertainment subset of the data, we'll initially take a look at the variables corresponding to the day of the week an article was published on, as well as the number of words in the title/body of the article and the number of images and videos included.  
  
Once we've completed our exploratory analysis, we'll select features that we think would be a good fit in various types of models. We'll compare and evaluate the models we build on several metrics to decide which model provides the most accurate forecasts of article popularity.  
  

# Data

```{r, message=FALSE}
# URL for the Online News Popularity Data Folder
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/00332/OnlineNewsPopularity.zip"

# Creates two temporary files
temp <- tempfile()
temp2 <- tempfile()

# Downloads the zipped folder from the URL and saves it in temp
download.file(url, temp)

# Unzips temp and saves it in temp2
unzip(zipfile = temp, exdir = temp2)

# Reads in the data from temp2 and saves it as data
data <- readr::read_csv(file.path(temp2, "OnlineNewsPopularity/OnlineNewsPopularity.csv"))

# Unlinks our temporary files
unlink(c(temp, temp2), force = TRUE)
```

```{r}
# Creates a new dataset "entertainment"
# Filters data for when the data channel is chosen
# Removes Variables that all start with data_channel_is_, url, and timedelta
# url and timedelta are non-predictive
dataSubset <- data %>% as_tibble() %>%
  filter(data_channel_is_entertainment == 1) %>%
  select(-c(url, timedelta, starts_with("data_channel_is_")))

# Creates a new character variable "day" that states what day it is. Derived from the binary variables for each day. 
# Factors the variable so we can use it as a categorical variable.
# Orders the factors in the order of the days of the week.
dataSubset2 <- dataSubset
dataSubset2$day <- ifelse(dataSubset$weekday_is_monday == 1, "Monday",
                            ifelse(dataSubset$weekday_is_tuesday == 1, "Tuesday",
                                   ifelse(dataSubset$weekday_is_wednesday == 1, "Wednesday",
                                          ifelse(dataSubset$weekday_is_thursday == 1, "Thursday",
                                                 ifelse(dataSubset$weekday_is_friday == 1, "Friday",
                                                        ifelse(dataSubset$weekday_is_saturday == 1, "Saturday",
                                                               ifelse(dataSubset$weekday_is_sunday == 1, "Sunday", "NA"))))))) %>%
                      as.factor() %>% 
                      ordered(levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))
```
  

# Summarizations

```{r plots}

# Histogram for shares
g <- ggplot(dataSubset, aes(x = shares)) + 
        geom_histogram(fill = "blue", binwidth = 5000) + 
        labs(x = "Shares", y = "Count", title = "Histogram of Shares")

# Scatterplot for n_tokens_title vs shares
g2 <- ggplot(dataSubset2, aes(x = n_tokens_title, y = shares)) + 
          geom_point() + 
          labs(x = "Number of Words in the Title", y = "Shares", title = "Title Word Count vs Shares") 

# Scatterplot for n_tokens_content vs shares
g3 <- ggplot(dataSubset2, aes(x = n_tokens_content, y = shares)) + 
          geom_point(color = "Green") + 
          labs(x = "Number of Words in the Content", y = "Shares", title = "Content Word Count vs Shares") 



# Added some new plots for number of images, videos and links vs. shares
g4 <- ggplot(dataSubset, aes(x = num_imgs, y = shares)) + geom_point(color = "Red") + labs(x = "Number of Images", y = "Shares", title = "Images vs Shares") 
g5 <- ggplot(dataSubset, aes(x = num_videos, y = shares)) + geom_point(color = "Aquamarine") + labs(x = "Number of Videos", y = "Shares", title = "Videos vs Shares") 
g6 <- ggplot(dataSubset, aes(x = num_hrefs, y = shares)) + geom_point(color = "Purple") + labs(x = "Number of Links", y = "Shares", title = "Links vs Shares") 

# We can move this up to where we have all the package info eventually but wanted to put it here for now
# Can put all of our scatter plots into a single grid so they're all lined up!
library(gridExtra)

grid.arrange(g, g2, g3, g4, g5, g6, ncol = 2, nrow = 3)

# Looking into plotting some of the distributions of the more obscure variables
g7 <- ggplot(dataSubset, aes(global_subjectivity)) + 
  geom_density(kernel = "gaussian", color = "Coral", fill = "Coral", alpha = .5) + labs(y = "Density", x = "Text Subjectivity", title = "Density Plot: Text Subjectivity") 

g8 <- ggplot(dataSubset, aes(global_sentiment_polarity)) + 
  geom_density(kernel = "gaussian", color = "Blue", fill = "Blue", alpha = .5) + labs(y = "Density", x = "Text Sentimental Polarity", title = "Density Plot: Text Sentimental Polarity")

g9 <- ggplot(dataSubset, aes(global_rate_positive_words)) + 
  geom_density(kernel = "gaussian", color = "Green", fill = "Green", alpha = .5) + labs(y = "Density", x = "Rate of Positive Words in the Content", title = "Density plot: Positive Words")

# Plots a Boxplot of number of shares per day of the week
g10 <- ggplot(dataSubset2, aes(x = day, y = shares)) + 
          geom_boxplot(fill = "grey") + 
          labs(x = "Day of the Week", y = "Shares", title = "Boxplot of Shares per Day") +
          scale_x_discrete(labels = c("Monday" = "Mon", "Tuesday" = "Tue", "Wednesday" = "Wed", "Thursday" = "Thu", "Friday" = "Fri", "Saturday" = "Sat", "Sunday" = "Sun"))

grid.arrange(g7, g8, g9, g10, ncol = 2, nrow = 2)

# Plot for every day of the week for Number of Words in the Content vs Shares
g11 <- ggplot(dataSubset2, aes(x = n_tokens_content, y = shares)) + 
          geom_point() + 
          facet_wrap(~ day) + 
          labs(x = "Number of Words in the Content", y = "Shares", title = "Content Word Count vs Shares for Every Day of the Week")
g11
```

```{r num_summaries}

# Calculates the minimum, average, median, maximum, and variance of Shares.
shareSum <- dataSubset %>% summarise(min = min(shares), avg = mean(shares), med = median(shares), max = max(shares), var = var(shares))

# Calculates the minimum, average, median, maximum, and variance of Text Subjectivity. 
globalSubSum <- dataSubset %>% summarise(min = min(global_subjectivity), avg = mean(global_subjectivity), med = median(global_subjectivity), max = max(global_subjectivity), var = var(global_subjectivity))

# Creates vector of summary statistic types.
word <- c("Minimum", "Average", "Median", "Maximum", "Variance")

# Prints out the summaries for Shares.
for(i in 1:5){
  print(paste0("The ", word[i], " of Shares is ", shareSum[i]))
}

# Prints out the summaries for Text Subjectivity.
for(i in 1:5){
  print(paste0("The ", word[i], " of Text Subjectivity is ", globalSubSum[i]))
}

# Creates a Contingency for Number of words in the title and if it's on a weekend or not.
kable(table(dataSubset$is_weekend, dataSubset$n_tokens_title), caption = "Counts for Number of Words in the Title for Weekends (0) or Weekdays (1)")

# Creates a Contingency for the Number of Keywords in the Metadata for every day of the week.
kable(table(dataSubset2$day, dataSubset2$num_keywords), caption = "Counts for Number of Keywords in the Metadata per Day of the Week")
```


# Modeling  
  
## Train/Test Split  
  
The code chunk below splits our data into our training and testing sets using `caret`:  
  
```{r}
# can move this eventually
library(caret)

set.seed(1024)

train_index <- createDataPartition(dataSubset$shares, p = 0.7, list = FALSE)

train <- dataSubset[train_index, ]
test <- dataSubset[-train_index, ]


```
  
## Linear Models  

Linear regression fits a linear equation to the data, attempting to model the relationship between two variables, the response and explanatory variables. This equation is most commonly found by using the method of least-squares, which minimizes the sum of squared residuals. These models are usually written as $Y_i = \beta_0 + \beta_1x_{i1}+ ... + \beta_px_{ip} + E_i$.
  
```{r steph_model_l, warning = FALSE}
# Creates a linear regression model with main effects and interaction terms.
fitMLR1 <- train(shares ~ .^2, data = train,
                method = "lm",
                preProcess = c("center", "scale"),
                trControl = trainControl(method = "cv", number = 5))
```

```{r tommy_model_l}
# Will spice up the factors here once we finalize which variables we're using.
fitMLR2 <- train(shares ~ ., data = train,
                method = "lm",
                preProcess = c("center", "scale"),
                trControl = trainControl(method = "cv", number = 5))
```
  
## Ensamble Models 

### Random Forest Model

Random forest is a an ensemble learning method that is an extension of the idea of the bagging method. Like the bagging method, the random forest algorithm uses bagging, also known as bootstrap aggregation, to resample from the data or a fitted model randomly. Then multiple decision trees are created from these samples to create an uncorrelated forest and the results are then averaged. Unlike bagging, random forest doesn't use all of it's predictors but uses a random subset of predictors for each bootstrap sample. If there is a strong predictor, it'll likely be used for every first split in bagging, so randomly subsetting predictors will reduce correlation of tree predictions in random forest models. 
  
```{r steph_model_e}
# Training the Random Forest model with 2 fold cross-validation with center and scaling the data via preprocess, and considering the values of mtry of 1 to a third of number of columns in the data with tuneGrid.
fitRF <- train(shares ~ ., data = train,
                method = "rf",
                preProcess = c("center", "scale"),
                trControl = trainControl(method = "cv", number = 2))
               
               #tuneGrid = expand.grid(mtry = c(1:round(ncol(dataSubset)/3))))

# Print out Model
fitRF

# Plot the hyperparameters
plot(fitRF)
```
  
### Boosted Tree Model  
  
A boosted tree model is a type of ensemble learning method that functions somewhat similarly to a random forest model but utilizes a technique known as boosting instead of bagging. Boosting as a whole attempts to correct errors created by individual trees in the ensemble method. In this case, the boosting process is iterative and each new decision tree that is made considers errors made by previous trees in order to increase the overall accuracy of the model.  
  

```{r tommy_model_e}
fitBT <- train(shares ~., data = train, method = 'gbm',
               preProcess = c("center", "scale"),
               trControl = trainControl(method = "cv", number = 5),
               verbose = FALSE)
# Model printout
fitBT

# Plot hyperparameters
plot(fitBT)
```

# Comparison

```{r comparisons, warning = FALSE}

# Predicts Shares using the models and test data
predFitMLR1 <- predict(fitMLR1, newdata = test)
predFitMLR2 <- predict(fitMLR2, newdata = test)
predFitRF <- predict(fitRF, newdata = test)
predFitBT <- predict(fitBT, newdata = test)

# Compares the predicted shares found above to the shares in the test dataset
postMLR1 <- postResample(predFitMLR1, obs = test$shares)
postMLR2 <- postResample(predFitMLR2, obs = test$shares)
postRF <- postResample(predFitRF, obs = test$shares)
postBT <- postResample(predFitBT, obs = test$shares)

# Creates the names of the different models 
modelNames <- c("Interaction and Main Effects Model", "Main Effects Model", "Random Forest Model", "Boosting Tree Model")

# row binds the post comparisons and makes it a dataframe
try <- rbind(postMLR1, postMLR2, postRF, postBT) %>% data.frame()

# Changes the row names to the names of the different models created above
rownames(try) <- modelNames

# Chooses the winner of the models by finding minimum RMSE
winner <- try %>% filter(RMSE == min(RMSE))

# Prints out the minimum
print(paste0("The winning model is the ", rownames(winner)))
```  
  
# Automation  
  
```{r automation}

channels <- data.frame(c("lifestyle","entertainment","bus","socmed","tech","world"))

output_files <- paste0(channels, ".html")

params_reports = lapply(channels, FUN = function(x){list(x)})

reports <- tibble(output_files, params_reports)

library(purrr)
pwalk(reports, rmarkdown::render, input="Project-2.Rmd")
```



